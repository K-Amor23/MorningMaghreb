# ğŸš€ Airflow Docker Stack for Casablanca Insights

This is a production-ready Docker setup for Apache Airflow that will automatically run your data scraping DAGs and keep your database fresh.

## ğŸ¯ What This Gives You

- **ğŸ”„ Automated Data Scraping**: Runs daily at 6:00 AM UTC
- **ğŸ“Š 78 Companies**: Automatically scraped from African Markets
- **ğŸ“ˆ Real-time Data**: Market prices, volume, financials
- **ğŸŒ Web UI**: Monitor and manage your pipelines
- **ğŸ“± Always-on**: Runs continuously in the background

## ğŸš€ Quick Start

### 1. Start the Stack
```bash
./start-airflow.sh
```

### 2. Access the Web UI
- **URL**: http://localhost:8080
- **Username**: admin
- **Password**: admin

### 3. Your DAGs Will Auto-Run
- **Daily at 6:00 AM UTC**: Company data refresh
- **Every 6 hours**: Market data updates
- **Weekly**: Newsletter generation

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow Web    â”‚    â”‚   Scheduler     â”‚    â”‚   Celery Worker â”‚
â”‚   (Port 8080)   â”‚    â”‚  (Runs DAGs)   â”‚    â”‚ (Executes Tasks)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
    PostgreSQL              Redis Queue            Your Scrapers
   (Metadata)            (Task Queue)         (African Markets, etc.)
```

## ğŸ“ What's Included

- **15+ DAGs**: Ready to run your scrapers
- **Database**: PostgreSQL for metadata
- **Queue**: Redis for task distribution
- **Volumes**: Persistent data storage
- **Environment**: Loads your `.env.local`

## ğŸ”§ Manual Setup (if needed)

### Start Services
```bash
docker-compose up -d
```

### Initialize Database
```bash
docker-compose exec airflow-webserver airflow db init
```

### Create Admin User
```bash
docker-compose exec airflow-webserver airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@morningmaghreb.com
```

### Start Scheduler
```bash
docker-compose exec airflow-webserver airflow scheduler
```

## ğŸ“Š Your DAGs

1. **`working_comprehensive_market_data`** - Main scraper (daily)
2. **`casablanca_etl_pipeline`** - ETL pipeline (daily)
3. **`smart_ir_scraping`** - IR reports (daily)
4. **`etf_bond_scraping`** - ETFs & bonds (daily)
5. **`weekly_newsletter`** - Newsletter (weekly)

## ğŸ‰ What Happens Next

Once running, your stack will:

1. **ğŸŒ… 6:00 AM UTC**: Scrape all 78 companies
2. **ğŸ“ˆ Real-time**: Update market data continuously
3. **ğŸ“Š Database**: Populate your Sky Garden database
4. **ğŸŒ Frontend**: Your website gets fresh data automatically
5. **ğŸ“§ Alerts**: Success/failure notifications

## ğŸ› ï¸ Troubleshooting

### Check Status
```bash
docker-compose ps
```

### View Logs
```bash
docker-compose logs airflow-webserver
docker-compose logs airflow-scheduler
docker-compose logs airflow-worker
```

### Restart Services
```bash
docker-compose restart
```

### Stop Everything
```bash
docker-compose down
```

## ğŸ”’ Security Notes

- **Default credentials**: admin/admin (change in production)
- **Port 8080**: Only accessible locally
- **Environment variables**: Loaded from your `.env.local`

## ğŸš€ Production Ready

This setup includes:
- âœ… **Health checks** for all services
- âœ… **Persistent volumes** for data
- âœ… **Load balancing** with Celery workers
- âœ… **Monitoring** and logging
- âœ… **Auto-restart** on failures

---

**ğŸ¯ Your data scraper will now run automatically 24/7!**
