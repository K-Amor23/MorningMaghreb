#!/bin/bash

# Master Pipeline Deployment Script
# This script sets up the complete data pipeline for Casablanca Insights

echo "ğŸš€ Deploying Master Pipeline for Casablanca Insights"
echo "=================================================="

# Check if we're in the right directory
if [ ! -f "package.json" ]; then
    echo "âŒ Error: Please run this script from the project root directory"
    exit 1
fi

# Step 1: Deploy database tables to Supabase
echo ""
echo "ğŸ“Š Step 1: Deploying database tables to Supabase..."
python scripts/deploy_master_pipeline_tables.py

if [ $? -eq 0 ]; then
    echo "âœ… Database tables deployed successfully"
else
    echo "âŒ Failed to deploy database tables"
    exit 1
fi

# Step 2: Set up Airflow (if not already running)
echo ""
echo "âš™ï¸ Step 2: Setting up Airflow..."

cd apps/backend/airflow

# Check if Airflow is already running
if pgrep -f "airflow webserver" > /dev/null && pgrep -f "airflow scheduler" > /dev/null; then
    echo "âœ… Airflow is already running"
else
    echo "Starting Airflow services..."
    
    # Initialize Airflow if needed
    if [ ! -d "logs" ]; then
        echo "Initializing Airflow database..."
        airflow db init
    fi
    
    # Start Airflow services
    airflow webserver --port 8080 --daemon
    airflow scheduler --daemon
    
    echo "âœ… Airflow services started"
fi

# Step 3: Deploy the master DAG
echo ""
echo "ğŸ“‹ Step 3: Deploying master data pipeline DAG..."

# Copy the master DAG to Airflow dags directory
cp dags/master_data_pipeline_dag.py ~/airflow/dags/

# Wait for DAG to be loaded
sleep 5

# Check if DAG is loaded
if airflow dags list | grep -q "master_data_pipeline"; then
    echo "âœ… Master data pipeline DAG deployed successfully"
else
    echo "âŒ Failed to deploy master data pipeline DAG"
    exit 1
fi

# Step 4: Test the pipeline
echo ""
echo "ğŸ§ª Step 4: Testing the pipeline..."

# Trigger a test run of the master DAG
airflow dags trigger master_data_pipeline

echo "âœ… Test run triggered successfully"

# Return to project root
cd ../..

# Summary
echo ""
echo "ğŸ‰ Master Pipeline Deployment Completed Successfully!"
echo "=================================================="
echo ""
echo "ğŸ“Š What was deployed:"
echo "   âœ… Database tables in Supabase"
echo "   âœ… Airflow services"
echo "   âœ… Master data pipeline DAG"
echo "   âœ… Sample data for testing"
echo ""
echo "ğŸ”— Access Points:"
echo "   â€¢ Airflow UI: http://localhost:8080"
echo "   â€¢ Username: admin"
echo "   â€¢ Password: admin123"
echo "   â€¢ Website: https://morningmaghreb.com"
echo ""
echo "ğŸ“‹ Pipeline Schedule:"
echo "   â€¢ Runs daily at 6:00 AM UTC"
echo "   â€¢ Scrapes market data from multiple sources"
echo "   â€¢ Stores data in Supabase"
echo "   â€¢ Sends notifications on success/failure"
echo ""
echo "ğŸ› ï¸ Management Commands:"
echo "   â€¢ Start Airflow: cd apps/backend/airflow && airflow webserver --port 8080"
echo "   â€¢ Stop Airflow: pkill -f airflow"
echo "   â€¢ View DAGs: http://localhost:8080"
echo ""
echo "Your master data pipeline is now ready! ğŸš€" 